{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #2 Programming Assignment\n",
    "CSCI567, Spring 2019<br>Victor Adamchik<br>**Due: 11:59 pm, Feb. 08, 2019**\n",
    "\n",
    "\n",
    "### Before you start: \n",
    "There is a known issue with vocareum that caused by vocareum website, not us. If you submit your homework twice in a row without refresh the page, the grade book score might not be updated. So I suggest you to refresh your page before doing another submission.<br>\n",
    "Also, this project have two part. You need to submit each part seperately on vocareum.<br>\n",
    "\n",
    "## Office Hour for Project Assignment 2\n",
    "Question for Part 1, we suggest you go to: 2nd Floor-201A: 1:00am - 3:00am, Thursday, January 31, 2019 for Yang, Fang<br>\n",
    "Question for Part 2, we suggest you go to: 2nd Floor-201A: 3:00am - 5:00am, Thursday, January 31, 2019 for Cheng-Ju, Lin<br>\n",
    "We will also hold office hour for the first week of February, we will announce the time shortly.<br><br>\n",
    "Also, you can post your question on Piazza under pa-2 folder. We will try our best to answer all questions as soon as possible. Everyone in the class is welcomed to answer these questions. We really appreciate that. Please make sure you read previous posts before creating a new post in case your question has been answered before. We prefer Piazza than emails. However, if you have any urgent issue, feel free to send an email to both of us. <br>\n",
    "Yang, Fang: yangfang@usc.edu<br>\n",
    "Cheng-Ju, Lin: chengjul@usc.edu<br>\n",
    "\n",
    "\n",
    "\n",
    "## Problem 1 Regression (40 points)\n",
    "For this Assignment you are asked to implement linear regression. You are asked to implement 6 python functions for linear regression in linear_regression.py.\n",
    "The input and output of the functions are specified in linear_regression.py. Note that we have already appended the column of 1â€™s to the feature matrix, so that you do not need to modify the data yourself. We have provide a linear_regression_test.py for you to run some of your code and do some testing. Read the code, run it and check the result during your implementation. <br>\n",
    "Submission: All you need to submit is linear_regression.py\n",
    "\n",
    "### Q1.1 Mean Square Error (5 points)\n",
    "First, You need to implement the mean sqaure error. This is very similar to the RSS you learned from the lecture note page 15 except you need take the arithmetic mean of the error. numpy already have a mean function for that, feel free to use it. There should be simple as one or two lines of code and there is only one thing you need to be careful: please check the dimension of the input w, X and y before you do matrix dot operation. You might need to take a transpose of some matrix in order to get the right shape.<br>\n",
    "Report the mean square error of the model on the give n test set, no rounding for the result. The type of the error is <class 'numpy.float64'>, you can do type(err) to check yours.\n",
    "* (5 points) ```TODO 1```\n",
    "You need to complete ```def mean_square_error(w, X, y)``` in ```linear_regression.py```\n",
    "\n",
    "### Q1.2 Linear Regression (5 points)\n",
    "Now, you need to train your model. In linear regression model, it means find the weight for each feature. Check the lecture note page 21 and find the expression of general least square solution. Again, be careful with your matrix shape. Also, use numpy inverse funciton so you don't need to create your own. The shape of your return w should be (12,) if you train your model with the data we provide you<br>\n",
    "Implement linear regression with no regularization and return the model parameters. Again, the implementation should be as simple as one or two lines of code. Use numpy instead of crazy nested for loop in your implementation. You don't need to worry about non-invertible matrix. We will take care of it in Q1.3.\n",
    "* (5 points) ```TODO 2```\n",
    "You need to complete ```def linear_regression_noreg(X, y)``` in ```linear_regression.py```\n",
    "\n",
    "Once you finish Q1.1 and Q1.2, you should be able to run your linear_regression_test.py. Read the output, check your dimension of w, and MSE for training, evaluation and testing dataset. The MSE for all of them should between 0.5~0.6. Otherewise, there must be something wrong with your implementation.\n",
    "\n",
    "### Q1.3 Handle Non-invertible Matrix (5 points)\n",
    "There are cases that during the calculation, the matrix are non-invertible. We manually created that situation in the data_loader.py line 40. We simply manully set one row and one column in the dataset to be zero. Thus, we will get similar result as letcure note page 29. Now, you need to implement the solution on lecture note page 31. Here is the rul: in this assignment, if the smallest absolute value of all eigenvalue of a matrix is smaller than $10^{-5}$, the matrix is non-invertible. If the matrix is non-invertible, keep adding $10^{-1}*I$ until it is invertible. You can use numpy functions to get eigen value for a matrix, get the absolute value ,find the minimum and create identity matrix, just search for them.\n",
    "* (10 points) ```TODO 3```\n",
    "You need to complete ```def linear_regression_invertible(X, y)``` in ```linear_regression.py```\n",
    "\n",
    "Once you finish Q1.3, run linear_regression_test.py again, the MSE should be between 0.5~0.6. Otherewise, there must be something wrong with your implementation.\n",
    "\n",
    "### Q1.4 Regularized Linear Regression (5 points)\n",
    "To prevent overfitting, we usually add regularization. For now, we will focus on L2 regularization, same as the one in lecture note page 50. Implement the regularized_linear_regression(X,y,lambda)\n",
    "* (10 points) ```TODO 4```\n",
    "You need to complete ```def regularized_linear_regression(X, y, lambd)``` in ```linear_regression.py```\n",
    "\n",
    "Once you finish Q1.4, run linear_regression_test.py again. Compare your result with Q1.3, did you find something interesting? Think about why or check the statement in red on lecture note page 50. \n",
    "\n",
    "### Q1.5 Tune the Regularized Linear Hypter_Parameter (10 points)\n",
    "Use your implementation from Q1.4, the regularized_linear_regression(X, y, lambd), try different lambds and find the best lambd to minimize the MSE. Use Xval and yval to tune your hyper-parameter lambda<br>\n",
    "tune_lambda(Xtrain, ytrain, Xval, yval, lambds). To help us grading, set your lambd as all power of 10 from $10^{-19}$ to $10^{19}$ . However, in real world, no one knows the best range of hyper-parameters like lambda.\n",
    "* (10 points) ```TODO 5```\n",
    "You need to complete ```def tune_lambda(Xtrain, ytrain, Xval, yval)``` in ```linear_regression.py```\n",
    "Once you finish Q1.5, run linear_regression_test.py again. You should find a lambda that is $1e^{-14}$. Otherewise, there must be something wrong with your implementation.\n",
    "\n",
    "\n",
    "### Q1.6 Polynomial Regression (10 points)\n",
    "We would like to introduce your another regression called polynomial regression. You can check the detail on wiki: https://en.wikipedia.org/wiki/Polynomial_regression However, you don't need to check that to finish the homework<br>\n",
    "Apply polinomial regression on the data, first try the second order function: $y = w_1*X + w_2*X^2$\n",
    "In this case, you should change the data $[X]$ to be $[X X^2]$<br>\n",
    "Implement the mapping_data(x, power) function. For the higher order function, Eg.$y = w_1*X + w_2*X^2 + w_3*X^3$, thus your the data $[X]$ to be $[X X^2 X^3...]$. Be careful here, the new X is still 2D matrix, of size(N, D*power). Use the insert function from numpy to quickly add data to original dataset.\n",
    "Obeserve how the mean square error change.<br>\n",
    "* ```TODO 6```\n",
    "You need to complete ```def mapping_data(X, power)``` in ```linear_regression.py```\n",
    "\n",
    "Once you finish Q1.6, you can run linear_regression_test.py, it will change the data with your mapping function, find corresponding w with , make prediction and report the mean square error again.<br>\n",
    "You will see that your MSE keep increasing while the power goes up. Think about why this happen.\n",
    "\n",
    "<br>\n",
    "This is end of part 1. Hopefully everything is clear and you didn't struggle too much. Now you should be able to take your next challenge, Part 2.\n",
    "<br>\n",
    "\n",
    "### Grading\n",
    "Your code will be graded on Vocareum with autograding script. For your reference, with my implementation the testing  process is no longer than 15s including generating the grade report, and the processing time shows on the terminal is less than 0.2s. As long as you code can finish grading on Vocareum, you should be good.<br>\n",
    "I can't provide any test case we used on the auto-grading script to you. However, I tried to help you indentify your problem with the printing statement. Please read them carefully.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "## Problem 2 Classification (60 points)\n",
    "For this assignment you are asked to implement both binary and multiclass classification using gradient descent to update weight and bias in each iteration. **PLEASE EDIT ```bm_classify.py``` ONLY**. Any other changes that cause system issues may lead to 0 point in this part.\n",
    "\n",
    "### Q2.1 Binary Classification - Perceptron vs. Logistic (28 points)\n",
    "\n",
    "In our lecture we have been discussing how to do binary classification using different loss. Now it is your time to implement perceptron loss and logistic loss for binary classification. In this problem you are given training set $ D=\\left \\{ \\left ( x_{n},y_{n} \\right )_{N}^{n=1} \\right \\} , y_{i}\\in \\left \\{ 0,1 \\right \\} \\forall i=1...N. $ Please note that the labels are not +1 or -1 as we discussed in lectures, so think carefully before you apply formula to this question. Your task is to learn a model $ w^{T}x + b $ that minimize the given loss. Please find the gradients of the two loss functions by yourself and apply **average** gradient descent to update $w, b$ in each iteration. For perceptron loss we define $Z = y_{n}\\left ( w^{T}x_{n}+b \\right ) > 0$ as correctly classified data.\n",
    "\n",
    "* (8 points) ```TODO 1```\n",
    "For perceptron loss that is find the minimizer of $$ F\\left ( w,b \\right ) = \\sum_{n=1}^{N}L_{perceptron}\\left ( y_{n}\\left ( w^{T}x_{n}+b \\right ) \\right ) = \\sum_{n=1}^{N}MAX\\left ( 0, -y_{n}\\left ( w^{T}x_{n}+b \\right )\\right ) $$\n",
    "\n",
    "* (8 points) ```TODO 2```\n",
    "For logistic loss that is find the minimizer of $$ F\\left ( w,b \\right ) = \\sum_{n=1}^{N}L_{logistic}\\left ( y_{n}\\left ( w^{T}x_{n}+b \\right ) \\right ) = \\sum_{n=1}^{N}ln\\left ( 1+e^{ -y_{n}\\left ( w^{T}x_{n}+b \\right )}\\right ) $$\n",
    "\n",
    "* (4 points) ```TODO 3```\n",
    "Also you will find out it is convenient to use sigmoid fuction $\\sigma \\left (z\\right)=(1+e^{-z})^{-1}$ for logistic loss, so please complete it. You can use this function in ```TODO 2```.\n",
    "\n",
    "* (4 points for each)```TODO 4``` ```TODO 5```\n",
    "After you learn the models, how do you know it is good enough? The intuitive way is to make some predictions to see if those predicted results are correct or not. Here we want you complete the prediction functions. It will be like something greater than 0 and something put into sigmoid function greater than 0.5. You may find out an interesting fact here.\n",
    "\n",
    "After you finish the 5 ```TODO```s above in ```bm_classify.py```, you can run ```binary.sh``` to test it. If your code is programmed correctly, you should see ```binary.out``` as an output file keeping taining and testing accurancies of two loss functions for three datasets. You will see how similar between perceptron loss and logistic loss. Quite interesting isn't it? Can you figure out why this happends? We will leave this open question to you.\n",
    "\n",
    "Two of the datasets you are going to do binary classification:\n",
    "* Synthetic data: <img src=\"Synthetic_data.png\" width=\"400\">\n",
    "* Two Moon data: <img src=\"Two_Moon_data.png\" width=\"400\">\n",
    "\n",
    "\n",
    "### Q2.2 Multiclass classification - SGD vs. GD (32 points)\n",
    "\n",
    "Well done! Ready to take our next challenge? Let's get into multiclass classification.\n",
    "In this question you are going to build a model to classify data into more than just two classes. Also you are going to implement both $SGD$ and $GD$ for multiclass classification and compare performances of the two approaches. Training dataset are similar to question Q2.1, but $ y_{i}\\in \\left \\{ 0,1,...,C-1 \\right \\} \\forall i=1...N.$ Your task is to learn models for multiclass classification based on minimizing logistic loss.\n",
    "\n",
    "Here let me give a short review of $SGD$.\n",
    "\n",
    "From our lecture we know multiclass logistic loss is $$ F\\left ( W \\right ) = \\sum_{n=1}^{N}ln\\left ( 1+\\sum_{k\\neq y_{n}} e^{\\left ( w_{k}-w_{y_{n}} \\right )^{T}x_{n}}\\right ).$$ Now we try to apply $SGD$. First we randomly pick a data $x_{n}$ and minimize logistic loss $$ g\\left ( W \\right ) = ln\\left ( 1+\\sum_{k\\neq y_{n}} e^{\\left ( w_{k}-w_{y_{n}} \\right )^{T}x_{n}}\\right ).$$ And then find the derivative $\\bigtriangledown _{w} g\\left ( W \\right )$, where $\\bigtriangledown _{w} g\\left ( W \\right )$ is a $CxD$ matrix. \n",
    "\n",
    "Let's look at each row k.\n",
    "\n",
    "If $k\\neq y_{n}$: $$\\bigtriangledown _{w_{k}} g\\left ( W \\right )= \\frac{e^{\\left ( w_{k}-w_{y_{n}} \\right )^{T}x_{n}}}{1+\\sum_{{k}'\\neq y_{n}} e^{\\left ( w_{{k}'}-w_{y_{n}} \\right )^{T}x_{n}}}x_{n}^{T} = \\mathbb{P}\\left ( k| x_{n};W\\right )x_{n}^{T}$$\n",
    "\n",
    "else: $$ \\bigtriangledown _{w_{k}} g\\left ( W \\right )= \\frac{-\\sum_{{k}'\\neq y_{n}} e^{\\left ( w_{{k}'}-w_{y_{n}} \\right )^{T}x_{n}}}{1+\\sum_{{k}'\\neq y_{n}} e^{\\left ( w_{{k}'}-w_{y_{n}} \\right )^{T}x_{n}}}x_{n}^{T} = \\left (\\mathbb{P}\\left ( y_{n}| x_{n};W\\right )  -1\\right )x_{n}^{T}$$\n",
    "\n",
    "where $\\mathbb{P}$ is softmax function.\n",
    "\n",
    "In the end, our update for $W$ is\n",
    "$$W \\leftarrow W-\\eta \\begin{bmatrix}\\mathbb{P}\\left ( y=0 |x_{n};W\\right )\n",
    "\\\\ \\vdots \n",
    "\\\\ \\mathbb{P}\\left ( y=y_{n} |x_{n};W\\right )-1\n",
    "\\\\ \\vdots \n",
    "\\\\ \\mathbb{P}\\left ( y=C-1 |x_{n};W\\right )\n",
    "\\end{bmatrix} x_{n}^{T}$$\n",
    "\n",
    "That is the whole idea of $SGD$ of logistic loss for multiclass classification.\n",
    "\n",
    "* (8 points) ```TODO 6```\n",
    "Complete the $SGD$ part in ```def multiclass_train```, and don't forget the bias $b$. To randomly pick a data from dataset, you can use ```np.random.choice``` one time in each iteration.\n",
    "\n",
    "* (16 points) ```TODO 7```\n",
    "Complete the $GD$ part in ```def multiclass_train```. Compare to $SGD$, $GD$ does not randomly pick a data $x_{n}$. Instead, $GD$ considers all training data points to compute derivative. Please think about how to compute $GD$, and again we want **average** gradient descent. Also there is a tricky point. When dataset is large, $GD$ will takes a large amount of time. How to reduce the time? Make sure you use **numpy programming** instead of nested for loops, otherwise you will not finish your test on Vocareum within the time limit.\n",
    "\n",
    "Hint 1: If you need to run a for loop for $N$ times to accumulate $C$x$D$ matrices, why not design an equivalent computation as a $C$x$N$ matrix dot another $NxD$ matrix.\n",
    "\n",
    "Hint 2: You may find it useful to use a special (one-hot) representation of the labels, where each label $y_{i}$ is represented as a row of zeros with a single 1 in the column, that corresponds to the class $y_{i}$. So this one-hot should be an $N$x$C$ matrix.\n",
    "\n",
    "Advice: To avoid numerical issues such as overflow and underflow caused by ```np.exp```. Let $x$ be a input vector to the softmax function. Use $\\tilde{x} = x - max(x)$ instead of using $x$ directly for the softmax function $x$ . That is, if you want to compute $f(x)_{i}$ , compute $f(\\tilde{x})_{i}=\\frac{exp(\\tilde{x}_{i})}{\\sum_{j=1}^{D}exp(\\tilde{x}_{j})}$ instead, which  is clearly mathematically equivalent but numerically more stable.\n",
    "\n",
    "\n",
    "* (8 points) ```TODO 8```\n",
    "You need to complete the predict function in ```def multiclass_predict```. For your convenience, you can use ```np.argmax```.\n",
    "\n",
    "After you complete ```TODO 6``` ```TODO 7``` ```TODO 8```, please run ```multiclass.sh``` to test your code. If your code is programmed correctly, you should see ```multiclass.out``` as a output file keeping processing time, taining and testing accurancies of $SGD$ and $GD$ for each given dataset. You shall see how fast $SGD$ process compared to $GD$, but its accuracy is hard to catch up with the other's. One more open question to leave you. Is there any chance that $SGD$ reach to the same accuracy as $GD$ does using less time?\n",
    "\n",
    "\n",
    "### Grading\n",
    "When you finish all ```TODO``` parts, please click submit button on Vocareum.\n",
    "Your code will be tested on some other datasets, and we will grade your solution based on how many testcases you pass for each ```TODO```.<br>\n",
    "If your answer takes a long time for grading, Vocareum will kill the process automatically. You are responsible to make sure your code can be graded successfully on Vocareum. It takes about 100 seconds to process the grading script on solution code, and a few more sceonds to get the grading report. Sometimes you may need to come back to check grading report later.\n",
    "\n",
    "That is all for this assignment.\n",
    "\n",
    "Two of the datasets you are going to do multicalss classification:\n",
    "* Clear smile data: <img src=\"Clear_smile_data.png\" width=\"400\">\n",
    "* Blur smile data: <img src=\"Blur_smile_data.png\" width=\"400\">\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
