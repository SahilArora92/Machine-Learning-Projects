{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 Hidden Markov Models (50 points)\n",
    "\n",
    "In Hidden Markov Model(HMM) part, you need first implement 5 important functions with HMM parametrers we given to you. Then you need to calculate HMM parameters by yourself and use it to solve Part-of-Speech Tagging problem.<br>\n",
    "After finishing the implementation, you can use hmm_test_script.py to test the correctness of your functions.\n",
    "\n",
    "## Office Hour\n",
    "Week 12<br>\n",
    "Apr. 5th Friday\tLVL 2nd Floor-201A\t10:00am to 12:00pm\tYixian Di yixiandi@usc.edu\n",
    "\n",
    "Week 13<br>\n",
    "Apr. 12th Friday LVL 2nd Floor-201A\t10:00am to 12:00pm\tYixian Di yixiandi@usc.edu\n",
    "\n",
    "Week 14<br>\n",
    "Apr. 18th Thursday LVL 2nd Floor-201A 10:00am to 12:00pm Yixian Di yixiandi@usc.edu\n",
    "\n",
    "## Grading guideline\n",
    "2.1 Implementation\n",
    "    1. forward function - 5 = 5*1 points\n",
    "    2. backward function - 5 = 5*1 points\n",
    "    3. sequence_prob function - 5 = 5*1 points\n",
    "    4. posterior_prob function - 5 = 5*1 points\n",
    "    5. viterbi function - 10 = 5*2 points\n",
    "    There are 5 sets of grading data. Each grading data includes paramaters(pi, A, B, obs_dict, state_dict, Osequence), which we will use to initialize HMM class and test your functions. To receive full credicts, your output of function 1-4 should within an error of 10^-8, your output of viterbi function should be identical with ours.\n",
    "2.2 Application to Part-of-Speech Tagging\n",
    "    1. model_training function - 10 = 10*(your_correct_pred_cnt/our_correct_pred_cnt)\n",
    "    2. speech_tagging function - 10 = 10*(your_correct_pred_cnt/our_correct_pred_cnt)\n",
    "    We will still use the dataset we given to you to grading this part(with a different random seed). We will train your model and ours based on same train_data. We will test model_training function and speech_tagging function separatly.\n",
    "    In order to check your model_training function, we will use 50 sentences from train_data to do Part-of-Speech Tagging. To receive full credits, your prediction accuracy should be identical with ours.\n",
    "    In order to check your speech_tagging function, we will use 50 sentences from test_data to do Part-of-Speech Tagging. To receive full credits, your prediction accuracy should be identical with ours.\n",
    "\n",
    "## What to submit\n",
    "hmm.py<br>\n",
    "speech_tagging.py\n",
    "\n",
    "## 2.1 Implementation (30 points)\n",
    "In 2.1, you are given parameters of a HMM and you will implement two procedures.<br>\n",
    "**(1) The Evaluation Problem**<br>\n",
    "Given a HMM and a sequence of observations, what is the probability that the observations are generated by the model?<br>\n",
    "Two algorithms usually be used for evaluation problem: the forward algorithm or the backwards algorithm (DO NOT confuse them with the forward-backward algorithm). Based on the result of forward algorithm and backward algorithm, you will be asked to calculate sequence probability and posterior probability<br>\n",
    "**(2) The Decoding Problem**<br>\n",
    "Given a model and a sequence of observations, what is the most likely state sequence in the model which produced the observation sequence. For decoding you will be asked to implement Viterbi algorithm.<br>\n",
    "\n",
    "### HMM Class\n",
    "In this project, we abstracted Hiddern Markov Model as a class. Each Hiddern Markov Model initialized with Pi, A, B, obs_dict and state_dict. HMM class has 5 inner functions: Forward function, backward function, sequence_prob function, posterior_prob function and viterbi function.\n",
    "```\n",
    "### You can add your own function or variables in HMM class, but you shouldn't change current exists. ###\n",
    "class HMM:\n",
    "    def __init__(self, pi, A, B, obs_dict, state_dict):\n",
    "        - pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(X_1 = s_i)\n",
    "        - A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(X_t = s_j|X_t-1 = s_i)\n",
    "        - B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, o] = P(Z_t = z_o| X_t = s_i)\n",
    "        - obs_dict: A dictionary mapping each observation symbol to their index in B\n",
    "        - state_dict: A dictionary mapping each state to their index in pi and A\n",
    "    # TODO:\n",
    "    def forward(self, Osequence):\n",
    "    # TODO:\n",
    "    def backward(self, Osequence):\n",
    "    # TODO:\n",
    "    def sequence_prob(self, Osequence):\n",
    "    # TODO:\n",
    "    def posterior_prob(self, Osequence):\n",
    "    # TODO:\n",
    "    def viterbi(self, Osequence):\n",
    "```\n",
    "\n",
    "### 2.1.1 Evaluation problem\n",
    "\n",
    "#### (a) Forward algorithm and backward algorithm (10 points)\n",
    "Here $\\lambda$ means the model. Please finish the implementation of forward() function and backward() function in hmm.py:\n",
    "- $\\alpha[i, t] = P(X_t = s_i, Z_{1:t} | \\lambda).$\n",
    "```\n",
    "def forward(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - self.pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(X_1 = s_i)\n",
    "    - self.A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(X_t = s_j|X_t-1 = s_i)\n",
    "    - self.B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, o] = P(Z_t = z_o| X_t = s_i)\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - alpha: (num_state*L) A numpy array alpha[i, t] = P(X_t = s_i, Z_1:Z_t | λ)\n",
    "    \"\"\"\n",
    "```\n",
    "- $\\beta[i, t] = P(Z_{t+1:T}|X_t = s_i, \\lambda).$\n",
    "```\n",
    "def backward(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - self.pi: (1*num_state) A numpy array of initial probailities. pi[i] = P(X_1 = s_i)\n",
    "    - self.A: (num_state*num_state) A numpy array of transition probailities. A[i, j] = P(X_t = s_j|X_t-1 = s_i)\n",
    "    - self.B: (num_state*num_obs_symbol) A numpy array of observation probabilities. B[i, o] = P(Z_t = z_o| X_t = s_i)\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - beta: (num_state*L) A numpy array beta[i, t] = P(Z_t+1:Z_T | X_t = s_i, λ)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### (b) Sequence probability (5 points)\n",
    "Based on your forward, backward function, you will calculate the sequence probability.(You can call forward function or backward function inside of sequence_prob function)\n",
    "$$P(Z_1, . . . , Z_T = O|\\lambda) = \\sum_{i=1}^{N}P(X_t = s_i, Z_{1:T} | \\lambda) = \\sum_{i=1}^{N}\\alpha[i, T]$$\n",
    "```\n",
    "def sequence_prob(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - prob: A float number of P(Z_1:Z_T | λ)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "#### (c) Posterior probability (5 points)\n",
    "The forward variable $\\alpha[i, t]$ and backward variable $\\beta[i, t]$ are used to calculate the posterior probability of a specific case. Now for t = 1...T and i = 1...N, we define posterior probability $\\gamma_t(i) = P(X_t = s_i|O,\\lambda)$ the probability of being in state $s_i$ at time t given the observation sequence O and the model $\\lambda$.<br>\n",
    "$$\\gamma_t(i) = \\frac{P(X_t = s_i, O|\\lambda)}{P(O|\\lambda)} = \\frac{P(X_t = s_i, Z_{1:t}|\\lambda)}{P(O|\\lambda)}$$<br>\n",
    "$$P(X_t = s_i, Z_{1:t}|\\lambda) = P(Z_{1:t}|X_t = s_i,\\lambda) \\cdot P(Z_{t+1:T}|X_t = s_i,\\lambda) \\cdot P(X_t = s_i|\\lambda) = \\alpha[i, t] \\cdot \\beta[i, t]$$\n",
    "Thus<br>\n",
    "$$\\gamma_t(i) = \\frac{\\alpha[i, t] \\cdot \\beta[i, t]}{P(O|\\lambda)}$$\n",
    "where<br>\n",
    "$$P(O|\\lambda) = \\sum_{i=1}^{N}\\alpha[i, T]$$\n",
    "Signature:\n",
    "```\n",
    "def posterior_prob(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - prob: (num_state*L) A numpy array of P(X_t = i|O, λ)\n",
    "    \"\"\"\n",
    "```\n",
    "You can use $\\beta_t(i)$ to find the most likely state at time t which is the state $Z_t=s_i$ for which $\\beta_t(i)$ is maximum. This algorithm works fine in the case when HMM is ergodic i.e. there is transition from any state to any other state. If applied to an HMM of another architecture, this approach could give a sequence that may not be a legitimate path because some transitions are not permitted. To avoid this problem Viterbi algorithm is the most common decoding algorithms used.\n",
    "\n",
    "#### (d) Viterbi algorithm (10 points)\n",
    "Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states. We want to compute the most likely state path that corresponds to the observation sequence O based HMM. Namely, $k^∗ = (k^∗_1,k^∗_2,··· ,k^∗_T) = argmax_k P(s_{k_1},s_{k_2},··· ,s_{k_T}|Z_1,Z_2,··· ,Z_T = O, \\lambda)$. <br>\n",
    "Signature:\n",
    "```\n",
    "def viterbi(self, Osequence):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - Osequence: (1*L) A numpy array of observation sequence with length L\n",
    "\n",
    "    Returns:\n",
    "    - path: A List of the most likely hidden state path k* (return state instead of idx)\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "## 2.2 Application to Speech Tagging ( 20 points)\n",
    "Part-of-Speech (POS) is a category of words (or, more generally, of lexical items) which have similar grammatical properties.(noun, verb, adjective, adverb, pronoun, preposition, conjunction, interjection, and sometimes numeral, article, or determiner.)<br>\n",
    "Part-of-Speech Tagging (POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its context.<br>\n",
    "Here you wiil use HMM to do POST. You will need to caculate the parameters(pi, A, B, obs_dict, state_dict) of HMM first and then apply Vitervi algorithm to do speech-tagging.\n",
    "\n",
    "### Dataset\n",
    "tags.txt: Universal Part-of-Speech Tagset\n",
    "\n",
    "| Tag | Meaning | English Examples |\n",
    "| :------ | :------ | :------ |\n",
    "| ADJ | adjective | new, good, high, special, big, local |\n",
    "| ADP | adposition | on, of, at, with, by, into, under |\n",
    "| ADV | adverb | really, already, still, early, now |\n",
    "| CONJ | conjunction | and, or, but, if, while, although |\n",
    "| DET | determiner, article | the, a, some, most, every, no, which |\n",
    "| NOUN | noun | year, home, costs, time, Africa |\n",
    "| NUM | numeral | twenty-four, fourth, 1991, 14:24 |\n",
    "| PRT | particle | at, on, out, over per, that, up, with |\n",
    "| PRON | pronoun | he, their, her, its, my, I, us |\n",
    "| VERB | verb | is, say, told, given, playing, would |\n",
    "| . | punctuation marks | . , ; ! |\n",
    "| X | other | ersatz, esprit, dunno, gr8, univeristy |\n",
    "\n",
    "sentences.txt: Including 57340 sentences which have already been tagged.<br>\n",
    "\n",
    "| Word | Tag |\n",
    "|:------:|:------:|\n",
    "| b100-48585 |\n",
    "| She | PRON |\n",
    "| had | VERB |\n",
    "| to | PRT |\n",
    "| move | VERB |\n",
    "| in | ADP |\n",
    "| some | DET |\n",
    "| direction | NOUN |\n",
    "| -- | . |\n",
    "| any | DET |\n",
    "| direction | NOUN |\n",
    "| that | PRON |\n",
    "| would | VERB |\n",
    "| take | VERB |\n",
    "| her | PRON |\n",
    "| away | ADV |\n",
    "| from | ADP |\n",
    "| this | DET |\n",
    "| evil | ADJ |\n",
    "| place | NOUN |\n",
    "| . | . |\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "In this part, we collect our dataset and tags with Dataset class. Dataset class includes tags, train_data and test_data. In both dataset include a list of senetences, each sentence is an object of Line class.<br>\n",
    "You only need to implement model_training function and Speech_tagging function. We have provide you the accuracy function, which you can use to compare your predict_tagging and true_tagging of a sentence. You can find the definition bellow.\n",
    "```\n",
    "### You can add your own functions or variables in Dataset class, but you shouldn't change current exists. ###\n",
    "class Dataset:\n",
    "    def __init__(self, tagfile, datafile, train_test_split=0.8, seed=112890):\n",
    "\t\tself.tags\n",
    "\t\tself.train_data\n",
    "\t\tself.test_data\n",
    "    def read_data(self, filename):\n",
    "    def read_tags(self, filename):\n",
    "    class Line:\n",
    "        def __init__(self, line, type):\n",
    "            self.id\n",
    "            self.words\n",
    "            self.tags\n",
    "            self.length\n",
    "        def show(self):\n",
    "# TODO:\n",
    "def model_training(train_data, tags)\n",
    "# TODO:\n",
    "def Speech_tagging(model, test_data, tags)\n",
    "def accuracy(predict_tagging, true_tagging)\n",
    "```\n",
    "\n",
    "### 2.2.1 Model traning (10 points)\n",
    "In this part, you will need to calculate the parameters of HMM model based on train_data and tags.<br>\n",
    "Signature:\n",
    "```\n",
    "def model_training(train_data, tags):\n",
    "\t\"\"\"\n",
    "\tInputs:\n",
    "\t- train_data: a list of sentences, each sentence is an object of Line class\n",
    "    - tags: a list of POS tags\n",
    "\n",
    "\tReturns:\n",
    "\t- model: an object of HMM class initialized with paramaters(pi, A, B, obs_dict, state_dict) you calculated based on traning dataset. We will use the object you returned to do speech tagging.\n",
    "\t\"\"\"\n",
    "```\n",
    "\n",
    "### 2.2.2 Speech_tagging (10 points)\n",
    "Based on HMM from 2.2.1, do speech tagging for each sentence on test data. Note when you meet a word which is unseen in training dataset. You need to modify the emission matrix and obs_dict of your current model in order to handle this case. You  will assume the emission probability from each state to a new unseen word is 10^-6(a very low probability).<br>\n",
    "```\n",
    "For example, in hmm_model.json, we use the following paramaters to initialize HMM:\n",
    "S = [\"1\", \"2\"]\n",
    "pi: [0.7, 0.3]\n",
    "A: [[0.8, 0.2], [0.4, 0.6]]\n",
    "B = [[0.5, 0, 0.4, 0.1], [0.5, 0.1, 0.2, 0.2]]\n",
    "Observations = [\"A\", \"C\", \"G\", \"T\"]\n",
    "If we find another observation symbol \"X\" in observation sequence, we will modify parameters of HMM as follows:\n",
    "S = [\"1\", \"2\"]\n",
    "pi: [0.7, 0.3]\n",
    "A: [[0.8, 0.2], [0.4, 0.6]]\n",
    "B = [[0.5, 0, 0.4, 0.1, 1e-6], [0.5, 0.1, 0.2, 0.2, 1e-6]]\n",
    "Observations = [\"A\", \"C\", \"G\", \"T\", \"X\"]\n",
    "```\n",
    "Remember you are not supposed to access test_data on model_training function, you need to implement this logic on speech_tagging function.<br>\n",
    "Signature:\n",
    "```\n",
    "def speech_tagging(test_data, model):\n",
    "\t\"\"\"\n",
    "\tInputs:\n",
    "\t- test_data: (1*num_sentence) a list of sentences, each sentence is an object of Line class\n",
    "\t- model: an object of HMM class\n",
    "\n",
    "\tReturns:\n",
    "\t- tagging: (num_sentence*num_tagging) a 2D list of output tagging for each sentences on test_data\n",
    "\t\"\"\"\n",
    "```\n",
    "\n",
    "### 2.2.3 Suggestion(0 points)\n",
    "This part won't be graded. In order to have a better understanding of HMM. Come up with one sentence by yourself and tagging it manually. Then run your forward function, backward function, seq_prob function, posterior_prob function and viterbi function on the model from 2.2.1. Print the result of each function, see if you can explain your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
