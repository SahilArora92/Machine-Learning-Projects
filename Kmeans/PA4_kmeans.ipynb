{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General instructions\n",
    "\n",
    "\n",
    "## High Level Description\n",
    "\n",
    "In this assignment you are asked to implement K-means clustering to identify main clusters in\n",
    "the data, use the discovered centroid of cluster for classification. Specifically, you will\n",
    "\n",
    " - Implement K-means clustering algorithm to identify clusters in a two-dimensional toy-dataset.\n",
    " - Implement image compression using K-means clustering algorithm.\n",
    " - Implement classification using the centroids identified by clustering on digits dataset.\n",
    " - Implement K-means++ clustering algorithm to identify clusters in a two-dimensional toy-dataset i.e. implement the kmeans++ function to compute the centers.\n",
    " \n",
    "NOTE: You only need to make changes in Kmeans.py and use KmeansTest.py for testing purposes and to see your results. You can find all TODO's sequentially in the Kmeans.py file. <br>\n",
    "Depending on your environment you may need to install the python library named, ”pillow”, which is used by matplotlib to process some of the images needed for this assignment. <br>\n",
    "You can install it by running ’pip3 install pillow’ in your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Guidelines (50 points):\n",
    "\n",
    "You are only required to submit Kmeans.py as that is the only file where you will be making any changes.\n",
    " - get_k_means_plus_plus_center_indices - 5 points (5 *1)\n",
    " - transform_image - 10 points (5 * 2 test cases) We are checking the MSE and the number of iterations for this\n",
    " - Kmeans( ) class on Toy dataset - 15 points (3 * 5 test cases) We are checking the centroid and membership for Kmeans and Kmeans++ \n",
    " - KmeansClassifier( ) class - 20 points (5 * 4 test cases) We are checking the accuracy and the centroids of the assignments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Office Hours\n",
    "\n",
    "Ashir Alam (ashirala@usc.edu) <br> \n",
    "April 5th  12:00pm- 1:00pm Leavey 301F <br> \n",
    "April 9th 10am - 12pm Leavey 301F <br>\n",
    "April 16th 10am - 12pm Leavey 301F <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for K-Means Clustering\n",
    "\n",
    "We will use 2 datasets - 2-D Toy Dataset and Digits datasets for K means part.<br>\n",
    "Toy Dataset is a two-dimensional dataset generated from 4 Gaussian distributions. We will use this\n",
    "dataset to visualize the results of our algorithm in two dimensions. You can find it in data_loader.py<br>\n",
    "We will use digits dataset from sklearn to test K-means based classifier and generate digits using\n",
    "Gaussian Mixture model. Each data point is a 8 × 8 image of a digit. This is similar to MNIST but less\n",
    "complex. There are 10 classes in digits dataset. <br>\n",
    "Link for Digits dataset: sklearn.datasets.digits http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K Means Clustering\n",
    "\n",
    " \n",
    "\n",
    "Recall that for a dataset $ x_1, . . . , x_N ∈ R^D $, the K-means distortion objective is: \n",
    "$$ F(\\{\\mu_k\\}, \\{r_{nk}\\}) = \\sum_{i=1}^N \\sum_{k=1}^K r_{nk} \\|\\mu_k- x_n\\|_2^2     \\qquad  (1)   $$   \n",
    "\n",
    "where $ µ_1, . . . , µ_K $ are centroids of the K clusters and $ r_{ik} ∈ {0, 1} $ represents whether example i belongs to cluster k. <br>\n",
    "Clearly, fixing the centroids and minimizing J over the assignment give\n",
    "\n",
    "$$ \\begin{equation}\n",
    "    \\hat r_{ik} = \\begin{cases}\n",
    "        1 & k = argmin_{k'} \\|\\mu_{k'}-x_n\\|_2^2 \\\\\n",
    "        0 & \\text{Otherwise.}\n",
    "    \\end{cases}\n",
    "    \\label{eq:opt_membership}\n",
    "\\end{equation} \\qquad  (2)$$\n",
    "\n",
    "On the other hand, fixing the assignment and minimizing $J$ over the centroids give\n",
    "$$ \\begin{equation}\n",
    "    \\hat \\mu_k =\\frac{ \\sum_{i=1}^N r_{nk} x_n}{\\sum_{i=1}^N r_{nk}}\n",
    "    \\label{eq:opt_mean}\n",
    "\\end{equation} \\qquad  (3) $$ \n",
    "\n",
    "What the K-means algorithm does is simply to alternate between these two steps.\n",
    "\n",
    "<img src = 'Algo1.png'>\n",
    "\n",
    "\n",
    "### 1.1 Implementing k-means++ algorithm\n",
    "\n",
    "\n",
    "Recall from lecture Kmeans++. Please refer to the algorithm below. In simple terms, cluster centers are initially chosen at random from the set of input observation vectors, where the probability of choosing vector x is high if x is not near any previously chosen centers. <br>\n",
    "\n",
    "Here is a one-dimensional example. Our observations are $ [0, 1, 2, 3, 4] $. Let the first center, $ c1 $, be 0. The probability that the next cluster center, $ c2 $, is x is proportional to $ ||c1-x||^2 $. So, $ P(c2 = 1) = 1a, P(c2 = 2) = 4a, P(c2 = 3) = 9a, P(c2 = 4) = 16a $, where $ a = 1/(1+4+9+16) $.<br>\n",
    "Suppose $ c2 = 4 $. Then, $ P(c3 = 1) = 1a, P(c3 = 2) = 4a, P(c3 = 3) = 1a $, where $ a = 1/(1+4+1) $. <br>\n",
    "For more insights, follow this: http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf <br>\n",
    "<img src = 'kmeans++.png' >\n",
    "\n",
    "Implement Algorithm by filling out the TODO parts in function **get_k_means_plus_plus_center_indices** of file **kmeans.py**. You can test this function on Vocareum separately. \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "def get_k_means_plus_plus_center_indices(n, n_cluster, x, generator=np.random):\n",
    "    :param n: number of samples in the data\n",
    "    :param n_cluster: the number of cluster centers required\n",
    "    :param x: data - numpy array of points\n",
    "    :param generator: random number generator from 0 to n for choosing the first cluster at random\n",
    "            The default is np.random here but in grading, to calculate deterministic results,\n",
    "            We will be using our own random number generator.\n",
    "\n",
    "\n",
    "    :return: the center points array of length n_clusters with each entry being index to a sample\n",
    "             which is chosen as centroid.\n",
    "           \n",
    "```\n",
    "If the generator is still not clear, its basically a np.random but helps us control the result during testing. SO wherever you would use np.random, use generator instead. <br> \n",
    "\n",
    "\n",
    "### 1.2 Implementing K-means clustering algorithm\n",
    "\n",
    "\n",
    "Implement Algorithm 1 by filling out the TODO parts (**fit** function) in class **KMeans** of file **kmeans.py**. Note the following:\n",
    "\n",
    " - Initialize means by picking self.n_cluster from N data points\n",
    " - Update means and membership until convergence or until you have made self.max_iter updates.\n",
    " - return (means, membership, number_of_updates)\n",
    " - If at some iteration, there exists a cluster k with no points assigned to it, then do not update the centroid of this cluster for this round.\n",
    " - While assigning a sample to a cluster, if there’s a tie (i.e. the sample is equidistant from two centroids), you should choose the one with smaller index (like what numpy.argmin does).\n",
    " - For each k, we are trying to compare based on the Euclidean distance. <br>\n",
    " \n",
    " \n",
    " ``` <br>\n",
    " Class KMeans:\n",
    "        Attr:\n",
    "            n_cluster - Number of cluster for kmeans clustering (Int)\n",
    "            max_iter - maximum updates for kmeans clustering (Int)\n",
    "            e - error tolerance (Float)\n",
    "            generator - random number generator from 0 to n for choosing the first cluster at random\n",
    "                The default is np.random here but in grading, to calculate deterministic results,\n",
    "                We will be using our own random number generator.\n",
    "            \n",
    "            def __init__(self, n_cluster, max_iter=100, e=0.0001, generator=np.random):\n",
    "                self.n_cluster = n_cluster\n",
    "                self.max_iter = max_iter\n",
    "                self.e = e\n",
    "                self.generator = generator\n",
    "              \n",
    "\n",
    "            def fit(self, x, centroid_func=get_lloyd_k_means):\n",
    "                Finds n_cluster in the data x\n",
    "                params: \n",
    "                x - N X D numpy array\n",
    "                centroid_func - To specify which algorithm we are using to compute the centers(Lloyd(regular) or Kmeans++) The default is Lloyd's Kmeans.\n",
    "                \n",
    "                returns: A tuple (centroids a n_cluster X D numpy array, y a length (N,) numpy array where cell i is the ith sample's assigned cluster, number_of_updates a Int)\n",
    "            Note: Number of iterations is the number of time you update the assignment\n",
    " \n",
    " ```\n",
    " \n",
    "After you complete the implementation, run KmeansTest.py to see the results of this on toy\n",
    "dataset. You should be able to see three images generated in plots folder. In particular, you can see\n",
    "toy dataset predicted labels.png and toy dataset real labels.png and compare the clusters identified by the algorithm against the real clusters. Your implementation should be able to recover the correct clusters sufficiently well. Representative images are shown in fig. 2. Red dots are cluster centroids.\n",
    "Note that color coding of recovered clusters may not match that of correct clusters. This is due to mis-match\n",
    "in ordering of retrieved clusters and correct clusters (which is fine). <br>\n",
    "\n",
    "\n",
    "<img src = 'PA4img.png' >\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Classification with k-means\n",
    "\n",
    "Another application of clustering is to obtain a faster version of the nearest neighbor algorithm. Recall that nearest neighbor evaluates the distance of a test sample from every training point to predict its class, which can be very slow. Instead, we can compress the entire training dataset to just the K centroids, where each centroid is now labeled as the majority class of the corresponding cluster. After this compression the prediction time of nearest neighbor is reduced from O(N) to just O(K) (see Algorithm 2 for the pseudocode). <br>\n",
    "<img src =  'Algo2.png' > \n",
    "<br>\n",
    "Complete the **fit** and **predict** function in **KMeansClassifier** in file **kmeans.py** . Once completed,\n",
    "run **KmeansTest.py** to evaluate the classifier on a test set (digits). For comparison, the script will also print accuracy of a logistic classifier and a nearest neighbor classifier. (Note: a naive K-means classifier may not do well but it can be an effective unsupervised method in a classification pipeline .) <br>\n",
    "\n",
    "Note: 1) break ties in the same way as in previous problems; 2) if some centroid doesn’t contain any\n",
    "point, set the label of this centroid as 0. <br>\n",
    "\n",
    "The prediction accuracy baseline is 0.77 for KMeans Lloyd(regular) algorithm and 0.72 for KMeans++ algorithm. Note: these differ on different datasets and in more cases Kmeans++ works better. \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "    Class KMeansClassifier:\n",
    "        \n",
    "        Attr:\n",
    "            n_cluster - Number of cluster for kmeans clustering (Int)\n",
    "            max_iter - maximum updates for kmeans clustering (Int)\n",
    "            e - error tolerance (Float)\n",
    "            generator - random number generator from 0 to n for choosing the first cluster at random\n",
    "            The default is np.random here but in grading, to calculate deterministic results,\n",
    "            We will be using our own random number generator.\n",
    "    \n",
    "\n",
    "        def __init__(self, n_cluster, max_iter=100, e=1e-6, generator=np.random):\n",
    "            self.n_cluster = n_cluster\n",
    "            self.max_iter = max_iter\n",
    "            self.e = e\n",
    "            self.generator = generator\n",
    "\n",
    "\n",
    "        def fit(self, x, y, centroid_func=get_lloyd_k_means):\n",
    "        \n",
    "            Train the classifier\n",
    "            params:\n",
    "                x - N X D size  numpy array\n",
    "                y - (N,) size numpy array of labels\n",
    "                centroid_func - To specify which algorithm we are using to compute the centers(Lloyd(regular) or Kmeans++) The default is Lloyd's Kmeans.\n",
    "            returns:\n",
    "                None\n",
    "            Stores following attributes:\n",
    "                self.centroids : centroids obtained by kmeans clustering (n_cluster X D numpy array)\n",
    "                self.centroid_labels : labels of each centroid obtained by \n",
    "                    majority voting (N,) numpy array) \n",
    "                    \n",
    "    \n",
    "         def predict(self, x):\n",
    "        \n",
    "            Predict function\n",
    "            params:\n",
    "                x - N X D size  numpy array\n",
    "            returns:\n",
    "                predicted labels - numpy array of size (N,)\n",
    "        \n",
    "       \n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "### 1.4 Image compression with K-means \n",
    "In this part, we will look at lossy image compression as an application of clustering. The idea is simply to treat each pixel of an image as a point $x_i$, then perform K-means algorithm to cluster these points, and finally replace each pixel with its centroid. <br> \n",
    "\n",
    "What you need to implement is to compress an image with K centroids given. Specifically, complete the\n",
    "function **transform_image** in the file **kmeans.py**. You have to reduce the image pixels and size by replacing each RGB values with nearest code vectors based on Euclidean distance. <br>\n",
    "After your implementation, and after completing Kmeans class, when you run KmeansTest.py, you should be able to see an image compressed_baboon.png in the plots folder. You can see that this image is distorted as compared to the original baboon.tiff. <br>\n",
    "The ideal result should take about 35-40 iterations and the Mean Square Error should be less than 0.0098. It takes about 1-2 minutes to complete normally.\n",
    "\n",
    "\n",
    "```\n",
    "def transform_image(image, code_vectors):\n",
    "\n",
    "        Quantize image using the code_vectors\n",
    "\n",
    "        Return new image from the image by replacing each RGB value in image with nearest code vectors (nearest in euclidean distance sense)\n",
    "\n",
    "        returns:\n",
    "            numpy array of shape image.shape\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
